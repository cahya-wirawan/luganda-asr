{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Luganda ASR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNhq2C/8kM42SKcHnbZuMng",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cahya-wirawan/luganda-asr/blob/main/Luganda_ASR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automatic Speech Recognition  (ASR) for Luganda\n",
        "This is the Python Notebook to describe how to train ASR model for Luganda, evaluate it and create submission file for the [Mozilla Luganda Automatic Speech Recognition](https://zindi.africa/competitions/mozilla-luganda-automatic-speech-recognition/).\n",
        "\n",
        "We use several python scripts to do the training/fine-tuning, evaluation, submission file creation:\n",
        "- run_finetuning.py\n",
        "- run_evaluation.py\n",
        "- run_submission.py "
      ],
      "metadata": {
        "id": "U5NlZqLxJRnZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check the availability of the GPU"
      ],
      "metadata": {
        "id": "54VzOwvrMAIv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bp-wU9KwNBoX",
        "outputId": "7acd980c-af5f-405d-d15a-481f504e6152"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Jan 20 13:04:10 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:            25G        725M        8.8G        1.2M         15G         24G\n",
            "Swap:            0B          0B          0B\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "!free -h"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparation of the Luganda ASR  source code "
      ],
      "metadata": {
        "id": "qXE7Fg4qhLFC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clone the source code from https://github.com/cahya-wirawan/luganda-asr.git"
      ],
      "metadata": {
        "id": "b_bjt6ibhhdA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!rm -rf /content/luganda-asr\n",
        "!git clone https://github.com/cahya-wirawan/luganda-asr.git"
      ],
      "metadata": {
        "id": "jbsU1U4-vw9N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bed0c11-dd39-4541-a4ba-2907cac8799c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'luganda-asr'...\n",
            "remote: Enumerating objects: 97, done.\u001b[K\n",
            "remote: Counting objects: 100% (97/97), done.\u001b[K\n",
            "remote: Compressing objects: 100% (78/78), done.\u001b[K\n",
            "remote: Total 97 (delta 48), reused 52 (delta 16), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (97/97), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/luganda-asr\n",
        "!git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJU0j4xFH21S",
        "outputId": "376630ee-a7c4-4fcb-c8f6-91465219daf2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/luganda-asr\n",
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the requirements"
      ],
      "metadata": {
        "id": "frnIcFhThxq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "f40I2tjtyS7f"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n",
        "\n",
        "Since we can't provide the Mozilla Common Voice dataset directly here due to its License, we ask you to download it from https://commonvoice.mozilla.org/lg/datasets directly and the same with test dataset from Zindi which can be downloaded from https://zindi.africa/competitions/mozilla-luganda-automatic-speech-recognition/data\n",
        "\n",
        "Please save all of it in your Google Drive directory, which we will mount it in this notebook. The directory structure in Google Drive should look as following after the mounting:\n",
        "\n",
        "/content/drive/MyDrive/Luganda\n",
        "- cv-corpus-7.0-2021-07-21-lg.tar.gz\n",
        "- SampleSubmission.csv\n",
        "- test_audio.zip\n",
        "- Test.csv\n"
      ],
      "metadata": {
        "id": "MAA1DQPVeBmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARvOjdMne1LY",
        "outputId": "bbec5a9e-1c0b-4e22-918c-913fd0e94871"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "luganda_data_dir = \"/content/drive/MyDrive/Luganda\"\n",
        "luganda_cv_corpus = f\"{luganda_data_dir}/cv-corpus-7.0-2021-07-21-lg.tar.gz\"\n",
        "luganda_test_audio_file = f\"{luganda_data_dir}/test_audio.zip\"\n",
        "luganda_test_file = f\"{luganda_data_dir}/Test.csv\"\n",
        "\n",
        "!ls -l $luganda_data_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlhD5Y1E9lVx",
        "outputId": "4a60101a-79da-42a9-e63c-d34d430ded4a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 1914072\n",
            "-rw------- 1 root root 1722860000 Jan 18 18:05 cv-corpus-7.0-2021-07-21-lg.tar.gz\n",
            "-rw------- 1 root root      91886 Jan 18 16:41 SampleSubmission.csv\n",
            "-rw------- 1 root root  236764554 Jan 18 16:39 test_audio.zip\n",
            "-rw------- 1 root root     292195 Jan 18 16:29 Test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "# Uncompressing the test_audio.zip and cv-corpus-7.0-2021-07-21-lg.tar.gz to the \n",
        "# directory /content/data.\n",
        "# It takes around 40s\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "## clean up the directory /content/data/\n",
        "!rm -rf /content/data\n",
        "\n",
        "!mkdir -p /content/data/zindi\n",
        "%cd /content/data/zindi\n",
        "!unzip -o $luganda_test_audio_file && cp $luganda_test_file .\n",
        "%cd /content/data\n",
        "!tar xvzf $luganda_cv_corpus "
      ],
      "metadata": {
        "id": "cJ0loUci9csm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"Content of /content/data:\" && ls -l /content/data\n",
        "!echo\n",
        "!echo \"Content of /content/data/zindi:\" && ls -l /content/data/zindi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jD9NdTcNJqEF",
        "outputId": "69654b41-8d57-4d18-9bd8-814bee588ff5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content of /content/data:\n",
            "total 8\n",
            "drwxr-xr-x 3 root root 4096 Jan 20 13:04 cv-corpus-7.0-2021-07-21\n",
            "drwxr-xr-x 4 root root 4096 Jan 20 13:04 zindi\n",
            "\n",
            "Content of /content/data/zindi:\n",
            "total 556\n",
            "drwxr-xr-x 3 root root   4096 Jan 20 13:04 __MACOSX\n",
            "drwxr-xr-x 2 root root 266240 Oct 22 06:14 test_audio\n",
            "-rw------- 1 root root 292195 Jan 20 13:04 Test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training\n",
        "\n",
        "We have fine-tuned the Facebook Wav2Vec2 model with the Luganda Common Voice dataset and stored it in https://huggingface.co/indonesian-nlp/wav2vec2-luganda. The model has been trained for 200 epochs in 4 hours and 30 minutes using 8 GPUs.\n",
        "\n",
        "We use the script run_finetuning.py to train the model. \n",
        "Due to resource and time limitation in Google Colab,\n",
        "we skip the model training here, but we run the evaluation and creation of the submission file. However, feel free to run following command for testing purpose:\n",
        "\n",
        "```!python run_finetuning.py finetuning_common_voice_1epoch.json```\n",
        "\n",
        "It will run the training for only one epoch which will take around 2 hours and 40 minutes in Google Colab.\n",
        "\n",
        "### Usage\n",
        "For training using single GPU\n",
        "``` \n",
        "% python run_finetuning.py <argument json file>\n",
        "``` \n",
        "For training using multi GPUs, for example 8 GPUs.\n",
        "``` \n",
        "% python -m torch.distributed.launch --nproc_per_node=8 run_finetuning.py <argument json file>\n",
        "``` \n",
        "Our model \"indonesian-nlp/wav2vec2-luganda\" has been trained using following command:\n",
        "``` \n",
        "% python -m torch.distributed.launch --nproc_per_node=8 run_finetuning.py finetuning_common_voice.json\n",
        "```\n",
        "\n",
        "List of possible arguments:\n",
        "``` \n",
        "% python run_finetuning.py -h\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "eYMN6cXWwN8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# We disable here the training to save times, but feel free to run it by \n",
        "# uncommenting the command \"!python run_finetuning.py finetuning_common_voice_1epoch.json\"\n",
        "%cd /content/luganda-asr\n",
        "\n",
        "!mkdir -p output\n",
        "#!python run_finetuning.py finetuning_common_voice_1epoch.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZI7R-qJwnIx",
        "outputId": "665bad64-9d4e-4762-b16d-c279c9141c7e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/luganda-asr\n",
            "CPU times: user 4.46 ms, sys: 7.3 ms, total: 11.8 ms\n",
            "Wall time: 110 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## N-grams Language Model\n",
        "\n",
        "We use the n-grams language model [KenLM](https://github.com/kpu/kenlm) to reduce further the Word Error Rate (WER)."
      ],
      "metadata": {
        "id": "21AVc8H2LcbD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we install the necessary linux packages to build the KenLM library"
      ],
      "metadata": {
        "id": "BR-zyW5rgtwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install build-essential libboost-all-dev cmake zlib1g-dev libbz2-dev liblzma-dev\n",
        "!sudo apt-get install libboost-all-dev libeigen3-dev"
      ],
      "metadata": {
        "id": "UNiCEM2FLgTb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we clone KenLM source and build it. The executables will be built in the directory \"kenlm/build/bin\""
      ],
      "metadata": {
        "id": "Hspfv7dkg6kq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "%cd /content/\n",
        "!if [ ! -d kenlm ]; then git clone https://github.com/kpu/kenlm; fi\n",
        "!cd kenlm && mkdir build && cd build && cmake .. && make -j 4"
      ],
      "metadata": {
        "id": "MW9vByc2LrlS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can prepare the text from the CV corpus and build the KenLM binary \"5gram.bin\""
      ],
      "metadata": {
        "id": "HR42WzkhhoE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/luganda-asr/\n",
        "\n",
        "!python run_lm_preparation.py -n dataset/common_voice -c lg -d /content/data/cv-corpus-7.0-2021-07-21 -o 5gram.txt\n",
        "!../kenlm/build/bin/lmplz -o 6 < \"5gram.txt\" > \"5gram.arpa\"\n",
        "!../kenlm/build/bin/build_binary \"5gram.arpa\" \"5gram.bin\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHHiM722MAkL",
        "outputId": "f0870f01-6f3d-4b0f-86fd-e002b6807228"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/luganda-asr\n",
            "Using custom data configuration lg-b5dd5bec651497f8\n",
            "Reusing dataset common_voice (/root/.cache/huggingface/datasets/common_voice/lg-b5dd5bec651497f8/7.0.0/d2815414a38db497e900b5788f75958a15449c23acb8371ded02d3ed746d6206)\n",
            "=== 1/5 Counting and sorting n-grams ===\n",
            "Reading /content/luganda-asr/5gram.txt\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "tcmalloc: large alloc 3449143296 bytes == 0x563e9278c000 @  0x7f80b19cf1e7 0x563e91b517e2 0x563e91aec4fe 0x563e91acb2eb 0x563e91ab7066 0x7f80afb68bf7 0x563e91ab8baa\n",
            "tcmalloc: large alloc 18395406336 bytes == 0x563f600e8000 @  0x7f80b19cf1e7 0x563e91b517e2 0x563e91b4080a 0x563e91b41248 0x563e91acb308 0x563e91ab7066 0x7f80afb68bf7 0x563e91ab8baa\n",
            "****************************************************************************************************\n",
            "Unigram tokens 315562 types 29210\n",
            "=== 2/5 Calculating and sorting adjusted counts ===\n",
            "Chain sizes: 1:350520 2:1345806592 3:2523387392 4:4037419776 5:5887904256 6:8074839552\n",
            "tcmalloc: large alloc 8074846208 bytes == 0x563e9278c000 @  0x7f80b19cf1e7 0x563e91b517e2 0x563e91b4080a 0x563e91b41248 0x563e91acb8d7 0x563e91ab7066 0x7f80afb68bf7 0x563e91ab8baa\n",
            "tcmalloc: large alloc 2523389952 bytes == 0x5640c4028000 @  0x7f80b19cf1e7 0x563e91b517e2 0x563e91b4080a 0x563e91b41248 0x563e91acbcdd 0x563e91ab7066 0x7f80afb68bf7 0x563e91ab8baa\n",
            "tcmalloc: large alloc 4037427200 bytes == 0x56415a6a6000 @  0x7f80b19cf1e7 0x563e91b517e2 0x563e91b4080a 0x563e91b41248 0x563e91acbcdd 0x563e91ab7066 0x7f80afb68bf7 0x563e91ab8baa\n",
            "tcmalloc: large alloc 5887909888 bytes == 0x5643a98ce000 @  0x7f80b19cf1e7 0x563e91b517e2 0x563e91b4080a 0x563e91b41248 0x563e91acbcdd 0x563e91ab7066 0x7f80afb68bf7 0x563e91ab8baa\n",
            "Statistics:\n",
            "1 29210 D1=0.715755 D2=1.11374 D3+=1.35288\n",
            "2 104286 D1=0.845218 D2=1.05038 D3+=1.44071\n",
            "3 135471 D1=0.876935 D2=0.595336 D3+=1.96891\n",
            "4 129896 D1=0.876117 D2=0.288674 D3+=2.21755\n",
            "5 111682 D1=0.860465 D2=0.228853 D3+=2.32792\n",
            "6 90717 D1=0.328856 D2=1.26226 D3+=2.73003\n",
            "Memory estimate for binary LM:\n",
            "type       kB\n",
            "probing 13617 assuming -p 1.5\n",
            "probing 16552 assuming -r models -p 1.5\n",
            "trie     6788 without quantization\n",
            "trie     3781 assuming -q 8 -b 8 quantization \n",
            "trie     6181 assuming -a 22 array pointer compression\n",
            "trie     3174 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
            "=== 3/5 Calculating and sorting initial probabilities ===\n",
            "Chain sizes: 1:350520 2:1668576 3:2709420 4:3117504 5:3127096 6:2902944\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
            "Chain sizes: 1:350520 2:1668576 3:2709420 4:3117504 5:3127096 6:2902944\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 5/5 Writing ARPA model ===\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Name:lmplz\tVmPeak:27630996 kB\tVmRSS:3429252 kB\tRSSMax:3429396 kB\tuser:0.724878\tsys:1.67172\tCPU:2.39662\treal:2.21412\n",
            "Reading 5gram.arpa\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation\n",
        "To test our fine-tuned model, we evaluate it with test split of Mozilla Common Voice dataset version 6.1 and 7.0. The evaluation also uses the Ken Language Model (KenLM) 5gram.bin we created from the text of Common Voice 7.0.\n",
        "\n",
        "\n",
        "### Usage\n",
        "Following is the command to evaluate test split of Mozilla Common Voice dataset using our model \"indonesian-nlp/wav2vec2-luganda\" and using the KenLM:\n",
        "``` \n",
        "% python run_evaluation.py -m indonesian-nlp/wav2vec2-luganda -n common_voice -c lg -k 5gram.bin --test_pct 100\n",
        "```\n",
        "List of possible arguments:\n",
        "``` \n",
        "% python run_evaluation.py -h\n",
        "usage: run_evaluation.py [-h] -m MODEL_NAME -n NAME -c CONFIG_NAME [-d DATA_DIR] [-b BATCH_SIZE] [-k KENLM] [--num_workers NUM_WORKERS] [-w BEAM_WIDTH] [--test_pct TEST_PCT] [--cpu]\n",
        "\n",
        "optional arguments:\n",
        "  -h, --help            show this help message and exit\n",
        "  -m MODEL_NAME, --model_name MODEL_NAME\n",
        "                        The wav2vec2 model name\n",
        "  -n NAME, --name NAME  The name of dataset\n",
        "  -c CONFIG_NAME, --config_name CONFIG_NAME\n",
        "                        The config name of the dataset\n",
        "  -d DATA_DIR, --data_dir DATA_DIR\n",
        "                        The directory contains the dataset\n",
        "  -b BATCH_SIZE, --batch_size BATCH_SIZE\n",
        "                        Batch size\n",
        "  -k KENLM, --kenlm KENLM\n",
        "                        Path to KenLM model\n",
        "  --num_workers NUM_WORKERS\n",
        "                        KenLM's number of workers\n",
        "  -w BEAM_WIDTH, --beam_width BEAM_WIDTH\n",
        "                        KenLM's beam width\n",
        "  --test_pct TEST_PCT   Percentage of the test set\n",
        "  --cpu                 Force to use CPU\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "zkbnzaStUCT3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation on Common Voice version 6.1\n",
        "\n",
        "We evaluate the ASR model \"indonesian-nlp/wav2vec2-luganda\" on the test split of Common Voice Dataset for Luganda (version 6.1). It takes around 6 minutes. We get here the Word Error Rate (WER) of **7.37%**."
      ],
      "metadata": {
        "id": "Yrwb5AURVGcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "%cd /content/luganda-asr\n",
        "!python run_evaluation.py -m indonesian-nlp/wav2vec2-luganda -n common_voice -c lg -k 5gram.bin --test_pct 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFsYf3VB5nyz",
        "outputId": "11459c8d-96c7-4a4e-e6ef-e1c2b0acbfe5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/luganda-asr\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Unigrams not provided and cannot be automatically determined from LM file (only arpa format). Decoding accuracy might be reduced.\n",
            "No known unigrams provided, decoding results might be a lot worse.\n",
            "Reusing dataset common_voice (/root/.cache/huggingface/datasets/common_voice/lg/6.1.0/5693bfc0feeade582a78c2fb250bc88f52bd86f0a7f1bb22bfee67e715de30fd)\n",
            "100% 584/584 [00:06<00:00, 83.93ex/s]\n",
            "100% 584/584 [01:08<00:00,  8.49ex/s]\n",
            "tcmalloc: large alloc 1253105664 bytes == 0x55e61f20c000 @  0x7f7ed9857615 0x55e4b392c4cc 0x55e4b3a0c47a 0x55e4b392f2ed 0x55e4b392f240 0x55e4b39a2973 0x55e4b3930afa 0x55e4b39a2d00 0x55e4b3930afa 0x55e4b399e915 0x55e4b399d9ee 0x55e4b3930bda 0x55e4b399e915 0x55e4b3930afa 0x55e4b399e915 0x55e4b399d9ee 0x55e4b3930bda 0x55e4b399e915 0x55e4b399d9ee 0x55e4b393148c 0x55e4b3931698 0x55e4b399ffe4 0x55e4b399d9ee 0x55e4b3930bda 0x55e4b399e915 0x55e4b3930afa 0x55e4b399e915 0x55e4b399d9ee 0x55e4b3930bda 0x55e4b399e915 0x55e4b399d9ee\n",
            "tcmalloc: large alloc 1566384128 bytes == 0x55e675ec2000 @  0x7f7ed9857615 0x55e4b392c4cc 0x55e4b3a0c47a 0x55e4b392f2ed 0x55e4b392f240 0x55e4b39a2973 0x55e4b3930afa 0x55e4b39a2d00 0x55e4b3930afa 0x55e4b399e915 0x55e4b399d9ee 0x55e4b3930bda 0x55e4b399e915 0x55e4b3930afa 0x55e4b399e915 0x55e4b399d9ee 0x55e4b3930bda 0x55e4b399e915 0x55e4b399d9ee 0x55e4b393148c 0x55e4b3931698 0x55e4b399ffe4 0x55e4b399d9ee 0x55e4b3930bda 0x55e4b399e915 0x55e4b3930afa 0x55e4b399e915 0x55e4b399d9ee 0x55e4b3930bda 0x55e4b399e915 0x55e4b399d9ee\n",
            "  0% 0/37 [00:00<?, ?ba/s]/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1093: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  return (input_length - kernel_size) // stride + 1\n",
            "100% 37/37 [03:12<00:00,  5.21s/ba]\n",
            "WER: 7.37%\n",
            "CPU times: user 2.32 s, sys: 406 ms, total: 2.73 s\n",
            "Wall time: 5min 7s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation on Common Voice version 7.0\n",
        "\n",
        "We evaluate the ASR model \"indonesian-nlp/wav2vec2-luganda\" on the test split of Common Voice Dataset for Luganda (version 7.0). It takes around 25 minutes. We get here the Word Error Rate (WER) of **7.53%**."
      ],
      "metadata": {
        "id": "J_zPIeviVr-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "%cd /content/luganda-asr\n",
        "!python run_evaluation.py -m indonesian-nlp/wav2vec2-luganda -n ./dataset/common_voice -c lg --data_dir /content/data/cv-corpus-7.0-2021-07-21 -k 5gram.bin --test_pct 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdXpjLbJRruk",
        "outputId": "5492d6c7-f14c-4e7f-d403-65896ac6bada"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/luganda-asr\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Unigrams not provided and cannot be automatically determined from LM file (only arpa format). Decoding accuracy might be reduced.\n",
            "No known unigrams provided, decoding results might be a lot worse.\n",
            "Using custom data configuration lg-b5dd5bec651497f8\n",
            "Reusing dataset common_voice (/root/.cache/huggingface/datasets/common_voice/lg-b5dd5bec651497f8/7.0.0/d2815414a38db497e900b5788f75958a15449c23acb8371ded02d3ed746d6206)\n",
            "100% 4276/4276 [00:39<00:00, 109.16ex/s]\n",
            "100% 4276/4276 [01:41<00:00, 42.28ex/s]\n",
            "tcmalloc: large alloc 1253105664 bytes == 0x55d96d552000 @  0x7f20c5048615 0x55d820d7a4cc 0x55d820e5a47a 0x55d820d7d2ed 0x55d820d7d240 0x55d820df0973 0x55d820d7eafa 0x55d820df0d00 0x55d820d7eafa 0x55d820dec915 0x55d820deb9ee 0x55d820d7ebda 0x55d820dec915 0x55d820d7eafa 0x55d820dec915 0x55d820deb9ee 0x55d820d7ebda 0x55d820dec915 0x55d820deb9ee 0x55d820d7f48c 0x55d820d7f698 0x55d820dedfe4 0x55d820deb9ee 0x55d820d7ebda 0x55d820dec915 0x55d820d7eafa 0x55d820dec915 0x55d820deb9ee 0x55d820d7ebda 0x55d820dec915 0x55d820deb9ee\n",
            "tcmalloc: large alloc 1566384128 bytes == 0x55d9b8060000 @  0x7f20c5048615 0x55d820d7a4cc 0x55d820e5a47a 0x55d820d7d2ed 0x55d820d7d240 0x55d820df0973 0x55d820d7eafa 0x55d820df0d00 0x55d820d7eafa 0x55d820dec915 0x55d820deb9ee 0x55d820d7ebda 0x55d820dec915 0x55d820d7eafa 0x55d820dec915 0x55d820deb9ee 0x55d820d7ebda 0x55d820dec915 0x55d820deb9ee 0x55d820d7f48c 0x55d820d7f698 0x55d820dedfe4 0x55d820deb9ee 0x55d820d7ebda 0x55d820dec915 0x55d820d7eafa 0x55d820dec915 0x55d820deb9ee 0x55d820d7ebda 0x55d820dec915 0x55d820deb9ee\n",
            "  0% 0/268 [00:00<?, ?ba/s]/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1093: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  return (input_length - kernel_size) // stride + 1\n",
            "100% 268/268 [13:00<00:00,  2.91s/ba]\n",
            "WER: 7.48%\n",
            "CPU times: user 6.61 s, sys: 1.09 s, total: 7.71 s\n",
            "Wall time: 15min 43s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission\n",
        "\n",
        "We will create the submission file \"submissions/luganda-asr.csv\"\n",
        "using the ASR model \"indonesian-nlp/wav2vec2-luganda\" on the test set\n",
        "provided by Zindi.\n",
        "It takes around 40 minutes\n",
        "\n",
        "### Usage\n",
        "```\n",
        "$ python run_submission.py -h\n",
        "usage: run_submission.py [-h] -m MODEL_NAME -d DATA_DIR -o OUTPUT_FILE [-b BATCH_SIZE] [-k KENLM] [-n NUM_WORKERS] [-w BEAM_WIDTH] [--test_pct TEST_PCT]\n",
        "\n",
        "optional arguments:\n",
        "  -h, --help            show this help message and exit\n",
        "  -m MODEL_NAME, --model_name MODEL_NAME\n",
        "                        The wav2vec2 model name\n",
        "  -d DATA_DIR, --data_dir DATA_DIR\n",
        "                        The directory contains the Zindi dataset (Train.csv, Test.csv and validated_dataset)\n",
        "  -o OUTPUT_FILE, --output_file OUTPUT_FILE\n",
        "                        The file name of the prediction result\n",
        "  -b BATCH_SIZE, --batch_size BATCH_SIZE\n",
        "                        Batch size\n",
        "  -k KENLM, --kenlm KENLM\n",
        "                        Path to KenLM model\n",
        "  -n NUM_WORKERS, --num_workers NUM_WORKERS\n",
        "                        KenLM's number of workers\n",
        "  -w BEAM_WIDTH, --beam_width BEAM_WIDTH\n",
        "                        KenLM's beam width\n",
        "  --test_pct TEST_PCT   Percentage of the test set\n",
        "```"
      ],
      "metadata": {
        "id": "-X1KGoJrWEiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "%cd /content/luganda-asr\n",
        "!mkdir -p submissions\n",
        "# For testing purpose, we can create submission file for only 1% of test dataset\n",
        "# which takes around 44s.\n",
        "#!python run_submission.py -o submissions/luganda-asr.csv -m indonesian-nlp/wav2vec2-luganda --data_dir /content/data/zindi -k 5gram.bin --test_pct 1\n",
        "\n",
        "# Following command creates submission file for 100% of test dataset which takes\n",
        "# around 40 minutes.\n",
        "!python run_submission.py -o submissions/luganda-asr.csv -m indonesian-nlp/wav2vec2-luganda --data_dir /content/data/zindi -k 5gram.bin\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xtbb0l2G6Q4k",
        "outputId": "00617c85-8aae-4a86-c4af-5e6347426dde"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/luganda-asr\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Unigrams not provided and cannot be automatically determined from LM file (only arpa format). Decoding accuracy might be reduced.\n",
            "No known unigrams provided, decoding results might be a lot worse.\n",
            "Using custom data configuration lg-e6ac6d8c877e86bb\n",
            "Downloading and preparing dataset zindi/lg to /root/.cache/huggingface/datasets/zindi/lg-e6ac6d8c877e86bb/1.0.0/48535cc6e254ea4fe8bd529ebd18452c88df0776030e5c69a33ea03b6bcf7436...\n",
            "manual_dir: /content/data/zindi\n",
            "datadir: /content/data/zindi\n",
            "Dataset zindi downloaded and prepared to /root/.cache/huggingface/datasets/zindi/lg-e6ac6d8c877e86bb/1.0.0/48535cc6e254ea4fe8bd529ebd18452c88df0776030e5c69a33ea03b6bcf7436. Subsequent calls will reuse this data.\n",
            "100% 7067/7067 [01:02<00:00, 113.82ex/s]\n",
            "  0% 0/442 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py:1093: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  return (input_length - kernel_size) // stride + 1\n",
            "100% 442/442 [20:25<00:00,  2.77s/it]\n",
            "\n",
            "The prediction result has been saved to submissions/luganda-asr.csv\n",
            "CPU times: user 8.09 s, sys: 1.31 s, total: 9.4 s\n",
            "Wall time: 21min 44s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "l_BH11z6AdGv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}